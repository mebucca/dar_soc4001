---
title: "Procesamiento avanzado de Bases de Datos en R"
subtitle: "Simulación en R e inferencia estadística"
author: "<br> Mauricio Bucca<br> Profesor Asistente, Sociología UC"
date: "[github.com/mebucca](https://github.com/mebucca)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default","default-fonts","gentle-r.css"]
    df_print: default
    nature:
      ratio: 16:9
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: true
      slideNumberFormat: "%current%"
editor_options: 
  chunk_output_type: console
---
layout: false


## Recapitulación

<br>

Iteración y automatización: 

  - Loops
  
  - "Functional programming"

---

## Hoy hablaremos de ...

<br>

Introducción a simulación e inferencia estadística: 

  - Variables aleatorias y simulación
  
  - Concepto de "sampling distribution"
  
  - Inferencia via simulación


---
class: fullscreen, left, middle, text-black
background-image: url("images/typewriter.jpg")

.huge[#Variables]
.huge[#aleatorias]

---
## Variables Aleatorias

Una .bold[variable aleatoria] es una variable cuyos valores son el resultado de un fenómeno aleatorio.

--

Ejemplo:
- Experimento: tirar 2 dados simultáneamente
- Posibles resultados (espacio muestral): $\{(1;1),(1;2), \dots, (5;6),(6;6)\}$


<br>
--
A partir de un experimento es posible definir una variedad de variables aleatorias. Por ejemplo:


- $Y$ es la variable que resulta de sumar el resultado de ambos dados:
  
  * $Y: \{2,3, \dots, 11,12 \}$


---

## Variables Aleatorias

Cada valor posible de una variable aleatoria tiene una cierta probabilidad de ocurrencia, denotada como $\mathbb{P}(Y=y)$.

--

.bold[Ejercicio rápido]:

- Experimento: tirar 2 dados justos simultáneamente

--

- $Y$ es la variable que resulta de sumar el resultado de ambos dados

--

.full-width[.content-box-red[
.bold[Pregunta]:
¿Cuál es la probabilidad que la variable $Y$ tome valor 12?
]
]

--

.full-width[.content-box-blue[
.bold[Respuesta]:
$$\mathbb{P}(Y=12) = \frac{1}{36}$$
]
]

---

### Distribución de una variable aleatoria

Cada valor posible de una variable aleatoria tiene una cierta probabilidad de ocurrencia. El conjunto de estas probabilidades se denomina la .bold[distribución] de la variable.

<br>
--

#### Función de probabilidad
La función que describe la distribución de una variable aleatoria se denomina .bold[función de probabilidad (pf)], denotada $f_{X}(x)$. 

--

- En el caso de variables discretas la función de probabilidad $f_{X}(x)$ entrega la probabilidad de que la variable $X$ tome valor $x$. Formalmente

$$f_{X}(x) = \mathbb{P}(X=x)$$

---
### Distribución de una variable aleatoria

Continuando con nuestro ejemplo:

- $Y$ es la variable que resulta de sumar el resultado de tirar dos dados justos

.pull-left[
```{r, echo=FALSE,message=FALSE, warning=FALSE}
library("tidyverse", "knitr")
library("viridis")

d1 = 1:6
d2 = 1:6

Y = NULL
for (i in d1) {
  for (j in d2) {
    sum_ij = i + j
    Y <- c(Y,sum_ij)
  }
}

ditrib_Y <- table(Y) %>% as_tibble() %>% mutate(p=round(n/sum(n),2)) %>% dplyr::select(-n)
names(ditrib_Y) <- c("y","P(Y=y)") 
knitr::kable(ditrib_Y, format = "markdown", align = 'lc')
```
]

--

.pull-right[
\begin{align}
  f_{Y}(y) =
  \begin{cases}
    \frac{1}{36}  & \quad \text{si } y=2 \text{ o } y=12\\
    \frac{2}{36}  & \quad \text{si } y=3 \text{ o } y=11\\
    \frac{3}{36}  & \quad \text{si } y=4 \text{ o } y=10\\
    \frac{4}{36}  & \quad \text{si } y=5 \text{ o } y=9\\
    \frac{5}{36}  & \quad \text{si } y=6 \text{ o } y=8\\
    \frac{6}{36}  & \quad \text{si } y=7 \\
    0             & \quad \text{otherwise}
  \end{cases}
\end{align}
]


---

### Distribución Bernoulli

- La distribución (discreta) más simple es una distribución Bernoulli-

--

-  Una variable aleatoria sigue una distribución de Bernoulli si solo puede tomar valores 0 o 1, con probabilidad $p$ y $q=1-p$, respectivamente.

--

Por ejemplo,

- Experimento: tirar una moneda

- Definamos la variable $X$ tal que $X=1$ si obtenemos Cara y $X=0$ si obtenemos Sello

--

$X$ es una variable Bernoulli con función de probabilidad:

\begin{align}
f_{X}(x) =
  \begin{cases}
    p  & \quad \text{si } x=1\\
    1 - p  & \quad \text{si } x=0 \\
    0 & \quad \text{otherwise}
  \end{cases}
\end{align}


---

### Distribución Bernoulli

.bold[Ilustración via simulación en] `R`

Tiremos una moneda con probabilidad de obtener "Cara" ( $1$ ) de 70% ( $p=0.7$ )

```{r}
set.seed(8786421) #<<
moneda <- rbinom(n=1, size=1, p=0.7) #<<
print(moneda)
```

--

Repitamos el proceso 100 veces ...

```{r}
set.seed(8786421)
monedas <- rbinom(n=100, size=1, p=0.7)
print(monedas)
```

---

### Distribución Bernoulli

.bold[Ejercicio rápido]:

- Experimento: tirar la misma moneda 2 veces

- Denotemos cada tiro como variables $X$ e $Y$

- $X$ e $Y$ distribuyen Bernoulli con parámetro $\mathbb{P}(X=1)=\mathbb{P}(Y=1)=p = 0.6$

- $X$ e $Y$ son independientes 

<br>
--

.content-box-red[
.bold[Pregunta]:
¿Cuál es la probabilidad de obtener "Sello" (0) en ambas ocasiones?
]

--

.content-box-blue[
.bold[Respuesta]:
$\mathbb{P}(X=0 \text{ y } Y=0) = (1-p)(1-p) = 0.4 \times 0.4 = 0.16$
]


---
### Distribución Bernoulli

--

Realicemos 1000 experimentos aleatorios ...

.pull-left[
```{r}
set.seed(8786421)
x <- rbinom(n=1000, size=1, p=0.6)
y <- rbinom(n=1000, size=1, p=0.6)

sim <- tibble(x=x,y=y); sim
```
]

--
Ahora contamos ...


.pull-right[
```{r}

sim <- sim %>% 
  mutate(z=if_else(x==0 & y==0,1,0)) #<<

table(sim$z)
prop.table(table(sim$z))
```
]

---
## Valor Esperado

El valor esperado de una variable es el análogo teórico de un promedio. Los posibles valores de la variable se ponderan por su probabilidad de ocurrencia. En el caso de variables discretas:

<br>
--

\begin{align}
\mathbb{E}(X) &= \sum_{i} x_{i} \times \mathbb{P}(X=x_{i}) \\
&\equiv  \sum_{i} x_{i} \times f_{X}(x_{i})
\end{align}

<br>
--
Es teórico porque esta información la podemos saber *a priori*, sin necesidad de datos. 


---
## Valor Esperado

Por ejemplo, supongamos que $Y$ es una variable que resulta de tirar un dado "justo".  ¿Cuál es el valor esperado de $Y$?

<br>
--

\begin{align}
\mathbb{E}(Y) &= \sum_{i}y_{i} \times \mathbb{P}(Y=y_{i})  \\ \\
     &=  1 \times  \frac{1}{6}+ 2 \times \frac{1}{6} + \dots + 6 \times \frac{1}{6}  \\ \\
     &= 3.5
\end{align}



---
## Varianza 

La varianza de una variable aleatoria es el análogo teórico de la varianza de los datos.
--
 Mide cuánta dispersión existe en torno al centro (la media). Formalmente, en el caso de variables aleatorias discretas:

<br>

$$\mathbb{Var}(X) = \sum_{i} \bigg( x_{i} - \mathbb{E}(X) \bigg)^{2} \times \mathbb{P}(X=x_{i})$$

---
## Varianza
Por ejemplo, si $Y$ es una variable que resulta de tirar un dado "justo", ¿cuál es la varianza de $Y$?

<br>
--

\begin{align}
\mathbb{Var}(Y) &= \sum_{i} \bigg( y_{i} - \mathbb{E}(Y) \bigg)^{2} \times \mathbb{P}(Y=y_{i})   \\ \\
     &=  (1 - 3.5)^{2} \times  \frac{1}{6} + (2-3.5)^{2} \times \frac{1}{6} + \dots + (6-3.5)^{2} \times \frac{1}{6}  \\ \\
     &= 2.91
\end{align}



---
### Distribuciones de probabilidad 

Cada distribución de probabilida tiene cuatro funciones asociadas cuyo .bold[prefijo] indica el *tipo de función* y el .bold[sufijo] indica la *distribución*. 

<br>
--

Ejemplo con distribución normal:

<br>
--

- .bold[d]`norm`: función de densidad 

--

- .bold[p]`norm`: función de densidad acumulada 

--

- .bold[q]`norm`: función de quantiles

--

- .bold[r]`norm`: muestro aleatorio desde un distribución dada.
--
  Nos concentraremos en esta función.

---
### Distribuciones de probabilidad: Uniforme

.pull-left[
```{r}
x <- runif(n=10^3, min=0, max=100)
x %>% matrix(ncol = 1)
```
]

--

.pull-right[
```{r, fig.height=5}
x %>% as_tibble() %>%
  ggplot(aes(x=value, fill="")) +
  geom_histogram() +
  scale_fill_viridis_d()
```
]

---
### Distribuciones de probabilidad: Bernoulli


.pull-left[
```{r}
x <- rbinom(n=10^3,size=1,p=0.7)
x %>% matrix(ncol = 1)
```
]

--

.pull-right[
```{r, fig.height=5}
x %>% as_tibble() %>%
  ggplot(aes(x=value, fill="")) +
  geom_bar() +
  scale_fill_viridis_d()
```
]

---
### Distribuciones de probabilidad: Binomial


.pull-left[
```{r}
x <- rbinom(n=10^3, size=10, p=0.7)
x %>% matrix(ncol = 1)
```
]

--

.pull-right[
```{r, fig.height=5}
x %>% as_tibble() %>%
  ggplot(aes(x=value, fill="")) +
  geom_bar() +
  scale_fill_viridis_d()
```
]

---
### Distribuciones de probabilidad: Normal


.pull-left[
```{r}
x <- rnorm(n=10^3, mean=10, sd=3)
x %>% matrix(ncol = 1)
```
]

--

.pull-right[
```{r, fig.height=5}
x %>% as_tibble() %>%
  ggplot(aes(x=value, fill="")) +
  geom_density() +
  scale_fill_viridis_d()
```
]

---
## Ejemplo: valor esperado y desviación estándar de variables aleatorias 

Supongamos que $X_{i}$ es la variable que resulta de tirar una moneda "justa". Participamos en un concurso que consiste en tirar la misma moneda 100 veces. El premio (G) es $ $1000$ de base más $ $10$ por cada "cara" obtenida ( $X_{i}=1$ ).

<br>
--

 - ¿Cuánto es el premio esperado?

 - ¿Cual es la desviación estándar del premio?

<br>
--


```{r, fig.height=5}
x <- rbinom(n=1,size=100,p=0.5); x
premio <- 1000 + 10*x; premio
```

---
## Valor Esperado y Desviación de variables aleatorios, ejemplo


 - ¿Cuánto es el premio esperado?

 - ¿Cual es la desviación estándar del premio?

--

```{r, fig.height=5}
concurso <- function(x) {
 x <- rbinom(n=1,size=100,p=0.5)
 premio <- 1000 + 10*x 
 return(premio) 
}

premios <- replicate(n=10^3,concurso() ); head(premios)
```

--

```{r, fig.height=5}
E_premio = mean(premios); E_premio
```

--

```{r, fig.height=5}
SD_premio = sd(premios); SD_premio
```
---
class: fullscreen, left, middle, text-black
background-image: url("images/typewriter.jpg")

.huge[#Inferencia]
.huge[#estadística]

---
##Inferencia Estadística

--

.bold[Configuración]:

--

- "Estimamos" una cantidad en bases a datos muestrales: 
  
  - Ejemplo, el promedio muestral $\bar{X}= \frac{x_1 + x_2 + \dots + x_{n}}{n}$ produce una estimación de la media poblacional $\mu_{X}$ 


--

- $\bar{X}$ es estimado en una muestra, no en la población, por tanto $\bar{X} \neq \mu_{X}$

--

- El estimador, aplicado a diferentes muestras, produce distintos resultados, pero sigue una distribución

--

- .bold[Inferencia:]

 - ¿Cómo podemos cuantificar la incertidumbre en torno a nuestras estimaciones?

--


- Para responder esta pregunta debemos conocer la .bold[sampling distribution] de nuestro estimador, especialmente su _variabilidad_ a través de "infinitas muestras".

---
##"Sampling distribution" de la media muestral


.bold[Caso canónico] es el _promedio muestral_, para el cual sabemos que: 

$$\bar{X} \sim \mathcal{N}(\mu,\frac{\sigma}{\sqrt{n}})$$

--

 - La desviación estándar del estimador (en este caso, $\frac{\sigma}{\sqrt{n}}$ ) es lo que denominamos .bold[error estándar (SE)].


<br>
<br>
--

.bold[¿De donde viene este resultado?] ....


---
##"Sampling distribution" de la media muestral

--

- Dado $\bar{X}= \frac{x_1 + x_2 + \dots + x_{n}}{n}$, donde $x_1, \dots, x_n$ vars independientes y con misma distribución 

--

- $\mathbb{Var}\big(\bar{X}\big)$ o  $\sqrt{\mathbb{Var}\big(\bar{X}\big)}$ "Error Estándar" (SE)?

--

  - $\mathbb{Var}\big(\bar{X}\big) = \mathbb{Var}\Big( \frac{\sum_{i} x_{i}}{n} \Big) = \mathbb{Var}\Big(\frac{x_{i} + ... + x_{n}}{n}\Big)$

--

  - $\mathbb{Var}\big(\bar{X}\big) = \frac{1}{n^2} \Big(\mathbb{Var}(x_{i}) + ... + \mathbb{Var}(x_{n})\Big)$

--

  - $\mathbb{Var}\big(\bar{X}\big) = \frac{1}{n^2}( \sigma^2 + ... + \sigma^2 ) = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n}$

--

.content-box-blue[
$$\sigma_{\bar{X}} =  \frac{\sigma}{\sqrt{n}}$$
]

---
##"Sampling distribution" de la media muestral

Más aún:

--


Importante resultado teórico: la sampling distribution de $\bar{X}$ es _asintóticamente_ normal:


.content-box-blue[
$$\bar{X} \sim \mathcal{N}(\mu,\sigma)$$
]

--

Podemos usar este resultado para construir un intervalo de confianza para $\bar{X}$, al (1 - $\alpha$)% de confianza. Para un nivel de significación estadística de $\alpha=0.05$,

<br>
\begin{align}
  95\% \text{ CI}_{\bar{X}} = \bar{X} \pm 1.96 \times SE
\end{align}

<br>
--

.bold[Nota importante]: cuando no conocemos los _verdaderos_ parámetros (casi siempre!), reemplazamos por sus valores estimados en la muestra.


---
##Ejemplo: "sampling distribution" de la media muestral

Retomando CASEN 2007, variable `y = log(ytotcor)`:


```{r, echo=FALSE, message=F, warning=F}
library("tidyverse")
library("broom")
library("viridis")

setwd(
  "~/Library/Mobile Documents/com~apple~CloudDocs/Teaching/ISUC/gentle-ggplot2/data/")

# leer archivo csv
data_casen_csv <- read_csv("sample_casen2017.csv")
data_casen_csv <- data_casen_csv %>% mutate(y = log(ytotcor)) %>% drop_na(y)
```

.pull-left[
.bold[Media y desviación estándar en muestra]:
]

.pull-right[
```{r, echo=F, message=F, warning=F}
stats <- c(mu= mean(data_casen_csv$y), sd= sd(data_casen_csv$y)); stats
```
]

--

.pull-left[
.bold[Error estándard de la media]:
]

.pull-right[
```{r, echo=F, message=F, warning=F}
summary(lm(data_casen_csv$y ~ 1))$coefficients
```
]

--

.pull-left[
.bold[Inferencia teórica]: 
]

.pull-right[
```{r, message=F, warning=F}
n = length(data_casen_csv$y)
se = stats["sd"]/sqrt(n); se 
ci = stats["mu"] + c(-1.96,1.96)*se; ci
```
]

--

.pull-left[
.bold[Inferencia via simulación en] `R`: 

]

.pull-right[
[click aquí](https://github.com/mebucca/dar_soc4001/blob/master/slides/class_14/class_14.R)
]

```{css, echo=FALSE}
.pull-right ~ * { clear: unset; }
.pull-right + * { clear: both; }
```

--

---
class: fullscreen,left, top, top, text-azzurro
background-image: url("images/bicicleta.jpg")

.huge[#R se aprende]
.huge[#usando y]
.huge[#preguntando]

---

![](images/stack-exchange-search.png)

---
class: inverse, middle

Presentacióny código en GitHub: <https://github.com/mebucca/data-analysis-in-R>

---
class: inverse, center, middle

.huge[
**Gracias!**
]

<br>
Mauricio Bucca <br>
https://mebucca.github.io/ <br>
github.com/mebucca





